{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gChG5lR5qbVG"
      },
      "source": [
        "# Clustering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJJHBy_yqbVP"
      },
      "source": [
        "Attribute Information:\n",
        "\n",
        "To construct the data, seven geometric parameters of wheat kernels were measured: \n",
        "1. area A, \n",
        "2. perimeter P, \n",
        "3. compactness C = 4*pi*A/P^2, \n",
        "4. length of kernel, \n",
        "5. width of kernel, \n",
        "6. asymmetry coefficient \n",
        "7. length of kernel groove. \n",
        "All of these parameters were real-valued continuous.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "id": "um3ltowuq4cJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e89a00a1-33ad-4980-e5cc-864aa54b345d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 31 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 36.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=a1c879b1e0ff6ee0113093fa9e709a8f3992d28cdb8b15424b4bf96a9050ee47\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "l1laP244qbVR"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('cluster').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wjdSrpLpqbVT"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Loads data.\n",
        "dataset = spark.read.csv(\"seeds_dataset.csv\",header=True,inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u61-tBR4qbVU",
        "outputId": "7d013057-427c-4089-bbe1-162793e9133d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15.26\n",
            "14.84\n",
            "0.871\n",
            "5.763\n",
            "3.312\n",
            "2.221\n",
            "5.22\n"
          ]
        }
      ],
      "source": [
        "for items in dataset.head():\n",
        "  print(items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UgCZqQqvqbVV",
        "outputId": "2c6ed371-8897-4d40-f555-4a0aef6478d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+--------------------+-------------------+------------------+---------------------+-------------------+\n",
            "|summary|              area|         perimeter|         compactness|   length_of_kernel|   width_of_kernel|asymmetry_coefficient|   length_of_groove|\n",
            "+-------+------------------+------------------+--------------------+-------------------+------------------+---------------------+-------------------+\n",
            "|  count|               210|               210|                 210|                210|               210|                  210|                210|\n",
            "|   mean|14.847523809523816|14.559285714285718|  0.8709985714285714|  5.628533333333335| 3.258604761904762|   3.7001999999999997|  5.408071428571429|\n",
            "| stddev|2.9096994306873647|1.3059587265640225|0.023629416583846364|0.44306347772644983|0.3777144449065867|   1.5035589702547392|0.49148049910240543|\n",
            "|    min|             10.59|             12.41|              0.8081|              4.899|              2.63|                0.765|              4.519|\n",
            "|    max|             21.18|             17.25|              0.9183|              6.675|             4.033|                8.456|               6.55|\n",
            "+-------+------------------+------------------+--------------------+-------------------+------------------+---------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SABAriyvqbVW"
      },
      "source": [
        "Formatting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "HYCaa3BEqbVX"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dxo7JY6LqbVZ",
        "outputId": "0e726ca6-4e8f-47fe-9e4f-25bd1e3de0fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['area',\n",
              " 'perimeter',\n",
              " 'compactness',\n",
              " 'length_of_kernel',\n",
              " 'width_of_kernel',\n",
              " 'asymmetry_coefficient',\n",
              " 'length_of_groove']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "Wh-RNx5AqbVc"
      },
      "outputs": [],
      "source": [
        "vec_assembler = VectorAssembler(inputCols = dataset.columns, outputCol='features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "5I3EL9puqbVd"
      },
      "outputs": [],
      "source": [
        "final_data = vec_assembler.transform(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbHCSpcrqbVd"
      },
      "source": [
        "Scaling the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "KQrF8JXGqbVe"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G2KDdPYQqbVf"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "vR9Gtxy1qbVh"
      },
      "outputs": [],
      "source": [
        "# Compute summary statistics by fitting the StandardScaler\n",
        "scalerModel = scaler.fit(final_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "h_iv0ByNqbVi"
      },
      "outputs": [],
      "source": [
        "# Normalize each feature to have unit standard deviation.\n",
        "final_data = scalerModel.transform(final_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MMQMEIvqbVi"
      },
      "source": [
        "Model building and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "rs2-0z1tqbVj"
      },
      "outputs": [],
      "source": [
        "# Trains a k-means model.\n",
        "kmeans = KMeans(featuresCol='scaledFeatures',k=3)\n",
        "model = kmeans.fit(final_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "viC403diqbVj",
        "outputId": "f014300b-267b-4cc2-a72c-4fa185719225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette with squared euclidean distance = 0.6018627534901196\n"
          ]
        }
      ],
      "source": [
        "# Make predictions \n",
        "predictions = model.transform(final_data)\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4ffT6Q10qbVk",
        "outputId": "dfe20cd8-6f16-4c3f-c34a-5f6f498cd3a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: \n",
            "[ 4.87257659 10.88120146 37.27692543 12.3410157   8.55443412  1.81649011\n",
            " 10.32998598]\n",
            "[ 6.31670546 12.37109759 37.39491396 13.91155062  9.748067    2.39849968\n",
            " 12.2661748 ]\n",
            "[ 4.06105916 10.13979506 35.80536984 11.82133095  7.50395937  3.27184732\n",
            " 10.42126018]\n"
          ]
        }
      ],
      "source": [
        "# Shows the result.\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sE8JLiz5qbVl",
        "outputId": "f8e45c7b-b8d8-45e2-8d34-46cfb28f886a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|prediction|\n",
            "+----------+\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         1|\n",
            "|         1|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         0|\n",
            "|         2|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.transform(final_data).select('prediction').show()"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}